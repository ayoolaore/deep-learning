{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 01-3, CSCI E-104\n",
    "### Finding model parameters using optimizer\n",
    "### This notebook is an illustration for chapter 5 of\n",
    "### Deep Learning with PyTorch by Eli Stevens, Luca Aantiga, Thomas Viehmann, Manning 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0,\n",
    "                    8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\n",
    "t_f = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9,\n",
    "                    33.9, 21.8, 48.4, 60.4, 68.4])\n",
    "t_fn = 0.1 * t_f \n",
    "\n",
    "add_size = 100 - t_c.size(0)\n",
    "\n",
    "\n",
    "t_c_added = torch.linspace(-25, 45, steps=add_size)\n",
    "\n",
    "t_f_added = (9.0 / 5.0) * t_c_added + 32.0\n",
    "t_c = torch.cat((t_c, t_c_added), 0)\n",
    "t_f = torch.cat((t_f, t_f_added), 0)\n",
    "\n",
    "def add_gaussian_noise(t, mean=0.0, std=3.0):\n",
    "    noise = torch.randn(t.size()) * std + mean\n",
    "    return t + noise\n",
    "t_f_noisy = add_gaussian_noise(t_f)\n",
    "t_c_noisy = add_gaussian_noise(t_c)\n",
    "\n",
    "t_f_train, t_f_val, t_c_train, t_c_val = train_test_split(\n",
    "    t_f_noisy, t_c_noisy, test_size=0.7, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the same model and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_f, w, b):\n",
    "    return w * t_f + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provided optimizers\n",
    "So far, we used vanilla gradient descent for optimization, which worked\n",
    "fine for our simple case.Tthere are several optimization strategies and\n",
    "tricks that can assist convergence, especially when models get complicatedNext, we will  to\n",
    "introduce the way PyTorch abstracts the optimization strategy away from user mined. This saves usrk\n",
    "of having to update each and every parameter to our model ourselves`. The` torch\n",
    "module `has a`  optim submodule where we can find classes implementing different\n",
    "optimization algorithms. Here’s an  listabridged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adafactor',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'Muon',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_adafactor',\n",
       " '_functional',\n",
       " '_muon',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every optimizer constructor takes a list of parameters (aka PyTorch tensors, typically\n",
    "with` requires_gra`d set to` Tru`e) as the first input. All parameters passed to the optimizer\n",
    "are retained inside the optimizer object so the optimizer can update their values\n",
    "and access the`ir g`rad attribute,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each optimizer exposes two methods: `zero_grad` and `step`. `zero_grad` zeroes the\n",
    "`\n",
    "gra`d attribute of all the parameters passed to the optimizer upon construction.` ste`p\n",
    "updates the value of those parameters according to the optimization strategy implemented\n",
    "by the specific optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here SGD stands for stochastic gradient descent. Actually, the optimizer itself is exactly a\n",
    "vanilla gradient descent (as long as the` momentu`m argument is set to 0.0, which is the\n",
    "default). The term stochastic comes from the fact that the gradient is typically obtained\n",
    "by averaging over a random subset of all input samples, called a minibatch. However, the\n",
    "optimizer does not know if the loss was evaluated on all the samples (vanilla) or a random\n",
    "subset of them (stochastic), so the algorithm is literally the same in the two cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.4873e-01, -8.0286e-04], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_f, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of `params` is updated upon calling step without us having to touch it ourselves!\n",
    "What happens is that the optimizer looks into `params.grad` and updates\n",
    "`params`, subtracting `learning_rate` times `grad` from it, exactly as in our former handrolled\n",
    "code.\n",
    "When using this code in a training loop we have to remember to zero out the gradients in every loop. Otherwise, the gradients\n",
    "would accumulate in the leaves at every call to `backward`. Below is the loop-ready code, with the extra\n",
    "`zero_grad` at the correct spot (right before the call to `backward`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (11) must match the size of tensor b (100) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m optimizer = optim.SGD([params], lr=learning_rate)\n\u001b[32m      5\u001b[39m t_p = model(t_fn, *params)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m loss = \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_c\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# The exact placement of this call is somewhat arbitrary. \u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# It could be earlier in the loop as well.\u001b[39;00m\n\u001b[32m     10\u001b[39m optimizer.zero_grad() \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mloss_fn\u001b[39m\u001b[34m(t_p, t_c)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss_fn\u001b[39m(t_p, t_c):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     squared_diffs = (\u001b[43mt_p\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_c\u001b[49m)**\u001b[32m2\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m squared_diffs.mean()\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (11) must match the size of tensor b (100) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_p = model(t_fn, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "# The exact placement of this call is somewhat arbitrary. \n",
    "# It could be earlier in the loop as well.\n",
    "optimizer.zero_grad() \n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated training loop now reads:#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, t_f, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p = model(t_f, *params) \n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860115\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957698\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate) # <1>\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 5000, \n",
    "    optimizer = optimizer,\n",
    "    params = params, # <1> \n",
    "    t_f = t_fn,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing other optimizers\n",
    "In order to test more optimizers, all we have to do is instantiate a different optimizer,\n",
    "sa`y Ad`am, instead o`f S`GD. The rest of the code stays as it isf.\n",
    "We won’t go into much detail abo`ut A`. It it is a more sophisticated\n",
    "optimizer in which the learning rate is set adaptively. In addition, it is a lot less\n",
    "sensitive to the scaling of the parameters—so insensitive that we can go back to  the original (non-normalized) input t_f, rather than t_fn, and even increase the learning rate to 1e-1.d Ada will handle it all.kusing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.612898\n",
      "Epoch 1000, Loss 3.086700\n",
      "Epoch 1500, Loss 2.928579\n",
      "Epoch 2000, Loss 2.927644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.5367, -17.3021], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-1\n",
    "optimizer = optim.Adam([params], lr=learning_rate) # <1>\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 2000, \n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_f = t_f, # We’re back to the original t_f as our input.\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Braking data into train and validate subset\n",
    "To sample a smaller validation set from all regions of the original dataset we usually shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3, 2, 6, 1, 7, 5, 8, 4, 9]), tensor([ 0, 10]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_f.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_indices, val_indices  # <1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_f = t_f[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_f = t_f[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "train_t_fn = 0.1 * train_t_f\n",
    "val_t_fn = 0.1 * val_t_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the trainign loop we calculate the loss of the validation data. Notice that we do not backpropagate through the validation data and we do not perform the `step` operation on the validation data. Trainign is done only on train(ing) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_f, val_t_f,\n",
    "                  train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_f, *params) # <1>\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "                             \n",
    "        val_t_p = model(val_t_f, *params) # <1>\n",
    "        val_loss = loss_fn(val_t_p, val_t_c)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward() # <2>\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch <= 3 or epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                  f\" Validation loss {val_loss.item():.4f}\")\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training we monitor both the training and the validation loss. If the validation loss stops falling, we are experiencing an overfitting issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 74.8975, Validation loss 104.9652\n",
      "Epoch 2, Training loss 34.0469, Validation loss 56.6435\n",
      "Epoch 3, Training loss 27.6024, Validation loss 47.3689\n",
      "Epoch 500, Training loss 7.3489, Validation loss 15.1187\n",
      "Epoch 1000, Training loss 3.7854, Validation loss 7.2841\n",
      "Epoch 1500, Training loss 3.1280, Validation loss 4.8208\n",
      "Epoch 2000, Training loss 3.0067, Validation loss 3.9292\n",
      "Epoch 2500, Training loss 2.9844, Validation loss 3.5769\n",
      "Epoch 3000, Training loss 2.9802, Validation loss 3.4312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.1394, -16.1405], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 3000, \n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    train_t_f = train_t_fn, # Since we’re using SGD again, we’re\n",
    "    val_t_f = val_t_fn, # back to using normalized inputs.\n",
    "    train_t_c = train_t_c,\n",
    "    val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are not being entirely fair to our model. The validation set is really small, so\n",
    "the validation loss will only be meaningful up to a point. In any case, we note that the\n",
    "validation loss is higher than our training loss, although not by an order of magnitude.\n",
    "We expect a model to perform better on the training set, since the model\n",
    "parameters are being shaped by the training set. Our main goal is to also see both the\n",
    "training loss and the validation loss decreasing. While ideally both losses would be\n",
    "roughly the same value, as long as the validation loss stays reasonably close to the\n",
    "training loss, we know that our model is continuing to learn generalized things about\n",
    "our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning autograd off on validation data\n",
    "Since we’re not ever calling backward\n",
    "on val_loss, We could in fact\n",
    "just cal`l mod`el an`d loss_`fn as plain functions, without tracking the computati through the autogradon.\n",
    "However optimized, building the autograd graph comes with additional costs that we\n",
    "could totally forgo during the validation pass, especially when the model has millions\n",
    "of parameters.\n",
    "In order to address this, PyTorch allows us to switch off autograd when we don’t\n",
    "need it, usi`ng the torch.`no_grad context m ger.12 We won’t see any meaningful\n",
    "advantage in terms of speed or memory consumption on our small problem. However,\n",
    "for larger models, the differences can add up. We can make sure this works by\n",
    "checking the val`ue of the req`uires_grad attrib `te on th`e val_loss tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_f, val_t_f,\n",
    "                  train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_f, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "\n",
    "        # Context manager here\n",
    "        with torch.no_grad(): # <1>\n",
    "            val_t_p = model(val_t_f, *params)\n",
    "            val_loss = loss_fn(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False # Cheking that requires_grad is set to False\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the related set_grad_enabled context, we can also condition the code to run\n",
    "with autograd enabled or disabled, according to a Boolean expression—typically indicating\n",
    "whether we are running in training or inference mode. We could, for instance,\n",
    "define a calc_forward function that takes data as input and runs model and loss_fn\n",
    "with or without autograd according to a Boolean train_is argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_forward(t_f, t_c, is_train):\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        t_p = model(t_f, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlasgn1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
